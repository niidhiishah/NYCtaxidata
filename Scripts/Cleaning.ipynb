{"cells":[{"cell_type":"code","execution_count":1,"id":"5787c4df","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-6275-m.us-central1-c.c.xenon-world-414321.internal:35953\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f5688a07290>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":2,"id":"45ede983","metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":3,"id":"bfafe559","metadata":{},"outputs":[],"source":["bucket = 'my-bigdata-project-ns-fhvhv'\n","file_list = ['fhvhv_tripdata_2020-01.parquet',\n","             'fhvhv_tripdata_2020-02.parquet',\n","             'fhvhv_tripdata_2020-03.parquet',\n","             'fhvhv_tripdata_2020-04.parquet',\n","             'fhvhv_tripdata_2020-05.parquet',\n","             'fhvhv_tripdata_2020-06.parquet',\n","             'fhvhv_tripdata_2020-07.parquet',\n","             'fhvhv_tripdata_2020-08.parquet',\n","             'fhvhv_tripdata_2020-09.parquet',\n","             'fhvhv_tripdata_2020-10.parquet',\n","             'fhvhv_tripdata_2020-11.parquet',\n","             'fhvhv_tripdata_2020-12.parquet',\n","             'fhvhv_tripdata_2021-01.parquet',\n","             'fhvhv_tripdata_2021-02.parquet',\n","             'fhvhv_tripdata_2021-03.parquet',\n","             'fhvhv_tripdata_2021-04.parquet',\n","             'fhvhv_tripdata_2021-05.parquet',\n","             'fhvhv_tripdata_2021-06.parquet',\n","             'fhvhv_tripdata_2021-07.parquet',\n","             'fhvhv_tripdata_2021-08.parquet',\n","             'fhvhv_tripdata_2021-09.parquet',\n","             'fhvhv_tripdata_2021-10.parquet',\n","             'fhvhv_tripdata_2021-11.parquet',\n","             'fhvhv_tripdata_2021-12.parquet',]\n","base_directory = f\"gs://my-bigdata-project-ns-fhvhv/Landing/\"\n","\n","for file_name in file_list:\n","    file_path = base_directory + file_name\n","    \n","    df = pd.read_parquet(file_path)"]},{"cell_type":"code","execution_count":4,"id":"4aa97187","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_32622/1145484515.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_cleaned[numeric_columns] = df_cleaned[numeric_columns].fillna(df_cleaned[numeric_columns].mean())\n","/opt/conda/miniconda3/lib/python3.11/site-packages/pandas/io/parquet.py:189: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n","  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"]},{"ename":"AttributeError","evalue":"'str' object has no attribute 'blob'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m df_cleaned\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_file_cleaned.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Upload the cleaned file to the GCS cleaned folder\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m blob_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mbucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblob\u001b[49m(my\u001b[38;5;241m-\u001b[39mbigdata\u001b[38;5;241m-\u001b[39mproject\u001b[38;5;241m-\u001b[39mns\u001b[38;5;241m-\u001b[39mfhvhv\u001b[38;5;241m/\u001b[39mCleaned \u001b[38;5;241m+\u001b[39m cleaned_file_name)\n\u001b[1;32m     26\u001b[0m blob_cleaned\u001b[38;5;241m.\u001b[39mupload_from_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_file_cleaned.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cleaning and transformation completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'blob'"]}],"source":["# Clean the data\n","# Remove records with missing data\n","df_cleaned = df.dropna()\n","\n","# Fill in missing values where appropriate\n","# For example, you can fill missing values in numeric columns with the mean\n","numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n","df_cleaned[numeric_columns] = df_cleaned[numeric_columns].fillna(df_cleaned[numeric_columns].mean())\n","\n","# Drop unneeded columns\n","# For example, if 'unnecessary_column' is not needed, you can drop it\n","unnecessary_column = 'dispatching_base_num'\n","df_cleaned = df_cleaned.drop(columns=[unnecessary_column])\n","\n","# Apply appropriate data types to columns\n","# For example, convert a column to datetime if it contains date information\n","df_cleaned['request_datetime',] = pd.to_datetime(df_cleaned['request_datetime'])\n","\n","\n","# Write the cleaned data to the cleaned folder as a Parquet file\n","cleaned_file_name = 'cleaned' + file_name\n","df_cleaned.to_parquet('local_file_cleaned.parquet')\n","\n","# Upload the cleaned file to the GCS cleaned folder\n","blob_cleaned = bucket.blob(my-bigdata-project-ns-fhvhv/Cleaned + cleaned_file_name)\n","blob_cleaned.upload_from_filename('local_file_cleaned.parquet')\n","\n","print(\"Data cleaning and transformation completed.\")"]},{"cell_type":"code","execution_count":null,"id":"58393b64","metadata":{},"outputs":[],"source":["# Import necessary libraries\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, mean\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.appName(\"NYC Taxi Data Cleaning\").getOrCreate()\n","\n","# Define bucket and file paths\n","bucket = 'my-bigdata-project-ns-fhvhv'\n","file_list = ['fhvhv_tripdata_2020-01.parquet',\n","             'fhvhv_tripdata_2020-02.parquet',\n","             'fhvhv_tripdata_2020-03.parquet',\n","             'fhvhv_tripdata_2020-04.parquet',\n","             'fhvhv_tripdata_2020-05.parquet',\n","             'fhvhv_tripdata_2020-06.parquet',\n","             'fhvhv_tripdata_2020-07.parquet',\n","             'fhvhv_tripdata_2020-08.parquet',\n","             'fhvhv_tripdata_2020-09.parquet',\n","             'fhvhv_tripdata_2020-10.parquet',\n","             'fhvhv_tripdata_2020-11.parquet',\n","             'fhvhv_tripdata_2020-12.parquet',\n","             'fhvhv_tripdata_2021-01.parquet',\n","             'fhvhv_tripdata_2021-02.parquet',\n","             'fhvhv_tripdata_2021-03.parquet',\n","             'fhvhv_tripdata_2021-04.parquet',\n","             'fhvhv_tripdata_2021-05.parquet',\n","             'fhvhv_tripdata_2021-06.parquet',\n","             'fhvhv_tripdata_2021-07.parquet',\n","             'fhvhv_tripdata_2021-08.parquet',\n","             'fhvhv_tripdata_2021-09.parquet',\n","             'fhvhv_tripdata_2021-10.parquet',\n","             'fhvhv_tripdata_2021-11.parquet',\n","             'fhvhv_tripdata_2021-12.parquet']\n","\n","base_directory = f\"gs://my-bigdata-project-ns-fhvhv/Landing/\"\n","cleaned_directory = f\"gs://my-bigdata-project-ns-fhvhv/Cleaned/\"\n","\n","# Loop through each file in the list\n","for file_name in file_list:\n","    file_path = base_directory + file_name\n","    \n","    # Load the data from the Parquet file\n","    df = spark.read.parquet(file_path)\n","    \n","    # Clean the data\n","    df_cleaned = df.dropna()  # Remove records with missing data\n","    \n","    # Check if there are records in the DataFrame before filling missing values\n","    if df_cleaned.count() > 0:\n","        # Identify numeric columns and fill missing values with column means\n","        numeric_columns = [c for c, t in df_cleaned.dtypes if t in ('double', 'int')]\n","        column_means = df_cleaned.select(*(mean(col(c)).alias(c) for c in numeric_columns)).collect()[0].asDict()\n","        df_cleaned = df_cleaned.fillna(column_means)\n","    \n","        # Drop unnecessary column\n","        df_cleaned = df_cleaned.drop(\"dispatching_base_num\")\n","    \n","        # Convert request_datetime to timestamp\n","        df_cleaned = df_cleaned.withColumn(\"request_datetime\", col(\"request_datetime\").cast(\"timestamp\"))\n","    \n","        # Write the cleaned data to Parquet format\n","        cleaned_file_path = cleaned_directory + 'cleaned_' + file_name\n","        df_cleaned.write.mode(\"overwrite\").parquet(cleaned_file_path)\n","    else:\n","        print(f\"No records found in {file_name}. Skipping data cleaning for this file.\")\n","\n","print(\"Data cleaning and transformation completed.\")\n"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}
